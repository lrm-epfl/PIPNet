{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f6c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import data\n",
    "import model\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c39b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pars = dict(\n",
    "                 # General parameters\n",
    "                 td = 512, # Number of points\n",
    "                 Fs = 10, # Sampling frequency\n",
    "                 debug = False, #Â Print data generation details\n",
    "    \n",
    "                 # Peak parameters\n",
    "                 pmin = 1, # Minimum number of Gaussians in a peak\n",
    "                 pmax = 10, # Maximum number of Gaussians in a peak\n",
    "                 ds = 0.03, # Spread of chemical shift values for each peak\n",
    "                 lw = [1e-2, 1e-1], # Linewidth range for Gaussians\n",
    "                 phase = 0., # Spread of phase\n",
    "    \n",
    "                 # Isotropic parameters\n",
    "                 nmin = 1, # Minimum number of peaks\n",
    "                 nmax = 10, # Maximum number of peaks\n",
    "                 shift_range = [1., 9.], # Chemical shift range\n",
    "                 positive = True, # Force the spectrum to be positive\n",
    "                 \n",
    "                 # MAS-dependent parameters\n",
    "                 mas_g_range = [1e4, 1e5], # MAS-dependent Gaussian broadening range\n",
    "                 mas_l_range = [1e4, 1e5], # MAS-dependent Lorentzian broadening range\n",
    "                 mas_s_range = [-1e4, 1e4], # MAS-dependent shift range\n",
    "                 mas_phase = 0.1, # Random phase range for MAS spectra\n",
    "                 peakwise_phase = True, # Whether the phase should be peak-wise or spectrum-wise\n",
    "                 encode_imag = False, # Encode the imaginary part of the MAS spectra\n",
    "                 nw = 8, # Number of MAS rates\n",
    "                 mas_w_range = [30000, 100000], # MAS rate range\n",
    "                 random_mas = False,\n",
    "                 encode_w = False, # Encode the MAS rate of the spectra\n",
    "    \n",
    "                 # Post-processing parameters\n",
    "                 noise = 0., # Noise level\n",
    "                 smooth_end_len = 10, # Smooth ends of spectra\n",
    "                 scale_iso = 0.8, # Scale isotropic spectra\n",
    "                 offset = 0., # Baseline offset\n",
    "                 norm_wr = True, # Normalize MAS rate values\n",
    "                 wr_inv = False # Encode inverse of MAS rate instead of MAS rate\n",
    "                )\n",
    "\n",
    "train_pars = dict(batch_size = 4, # Dataset batch size\n",
    "                  num_workers = 8, # Number of parallel processes to generate data\n",
    "                  checkpoint = 100, # Perform evaluation after that many batches\n",
    "                  n_eval = 100, # Number of batches in the evaluation\n",
    "                  max_checkpoints = 100, # Maximum number of checkpoints before finishing training\n",
    "                  out_dir = \"../data/Ensemble_PIPNet_test/\", # Output directory\n",
    "                  change_factor = {50: 100., 90: 10.}, # Checkpoints where \n",
    "                  avg_models = False,\n",
    "                  device = \"cpu\",\n",
    "                  monitor_end = \"\\r\"\n",
    "                 )\n",
    "\n",
    "model_pars = dict(n_models = 5,\n",
    "                  input_dim = 1,\n",
    "                  hidden_dim = 64,\n",
    "                  kernel_size = [1, 3, 5],\n",
    "                  num_layers = 3,\n",
    "                  final_kernel_size = 1,\n",
    "                  batch_input = 4,\n",
    "                  bias = True,\n",
    "                  final_bias = True,\n",
    "                  return_all_layers = False,\n",
    "                  final_act = \"sigmoid\",\n",
    "                 )\n",
    "    \n",
    "if not os.path.exists(train_pars[\"out_dir\"]):\n",
    "    os.mkdir(train_pars[\"out_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b2fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.PIPDataset(**data_pars)\n",
    "\n",
    "net = model.ConvLSTMEnsemble(**model_pars).to(train_pars[\"device\"])\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "# L1 loss\n",
    "#loss = model.CustomLoss(exp=1., offset=1., factor=0., out_factor=0.)\n",
    "# L2 loss\n",
    "#loss = model.CustomLoss(exp=2., offset=1., factor=0., out_factor=0.)\n",
    "# Custom loss\n",
    "loss = model.CustomLoss(exp=1., offset=1., factor=1000., out_factor=0.)\n",
    "\n",
    "sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dc0e8",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafee3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "    Training batch  100: loss =  3.3816e+00, mean loss =  3.1411e+00, lr =  1.0000e-03...\n",
      "  Checkpoint reached, evaluating the model...\n",
      "    Validation batch  100: loss =  3.8290e+00, mean loss =  3.1374e+00...\n",
      "  End of evaluation.\n",
      "    Training batch  200: loss =  2.1448e+00, mean loss =  3.0609e+00, lr =  1.0000e-03...\n",
      "  Checkpoint reached, evaluating the model...\n",
      "    Validation batch  100: loss =  2.5115e+00, mean loss =  3.0323e+00...\n",
      "  End of evaluation.\n",
      "    Training batch  263: loss =  2.3084e+00, mean loss =  2.8316e+00, lr =  1.0000e-03...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1200f3790>\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c33ce951c577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Work/PIP/PIPNet/1D/src/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, net, opt, loss, sch, train_pars)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.train(dataset, net, opt, loss, sch, train_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ecbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

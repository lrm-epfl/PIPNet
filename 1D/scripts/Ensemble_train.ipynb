{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7479fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import data\n",
    "import model\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f8cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pars = dict(\n",
    "                 # General parameters\n",
    "                 td = 256, # Number of points\n",
    "                 Fs = 12800, # Sampling frequency\n",
    "                 debug = False, #Â Print data generation details\n",
    "\n",
    "                 # Peak parameters\n",
    "                 pmin = 1, # Minimum number of Gaussians in a peak\n",
    "                 pmax = 1, # Maximum number of Gaussians in a peak\n",
    "                 ds = 0.03, # Spread of chemical shift values for each peak\n",
    "                 lw = [[5e1, 2e2], [1e2, 2e3]], # Linewidth range for Gaussians\n",
    "                 iso_p = [0.9, 0.1],\n",
    "                 iso_p_peakwise = True,\n",
    "                 phase = 0., # Spread of phase\n",
    "\n",
    "                 # Isotropic parameters\n",
    "                 nmin = 1, # Minimum number of peaks\n",
    "                 nmax = 15, # Maximum number of peaks\n",
    "                 shift_range = [2000., 10000.], # Chemical shift range\n",
    "                 positive = True, # Force the spectrum to be positive\n",
    "\n",
    "                 # MAS-dependent parameters\n",
    "                 mas_g_range = [[1e10, 1e11], [1e10, 5e11]], # MAS-dependent Gaussian broadening range\n",
    "                 mas_l_range = [[1e7, 1e8], [1e7, 5e8]], # MAS-dependent Lorentzian broadening range\n",
    "                 mas_s_range = [[-1e7, 1e7], [-1e7, 1e7]], # MAS-dependent shift range\n",
    "                 mas_p = [0.9, 0.1],\n",
    "                 mas_phase = 0.1, # Random phase range for MAS spectra\n",
    "                 peakwise_phase = True, # Whether the phase should be peak-wise or spectrum-wise\n",
    "                 encode_imag = False, # Encode the imaginary part of the MAS spectra\n",
    "                 nw = 4, # Number of MAS rates\n",
    "                 mas_w_range = [30000, 100000], # MAS rate range\n",
    "                 random_mas = True,\n",
    "                 encode_w = True, # Encode the MAS rate of the spectra\n",
    "\n",
    "                 # Post-processing parameters\n",
    "                 noise = 0., # Noise level\n",
    "                 smooth_end_len = 10, # Smooth ends of spectra\n",
    "                 scale_iso = 0.8, # Scale isotropic spectra\n",
    "                 offset = 0., # Baseline offset\n",
    "                 norm_wr = True, # Normalize MAS rate values\n",
    "                 wr_inv = False # Encode inverse of MAS rate instead of MAS rate\n",
    "                )\n",
    "\n",
    "loss_pars = dict(srp_w = 1.,\n",
    "                 srp_exp = 1.,\n",
    "                 srp_offset = 1.,\n",
    "                 srp_fac = 100.,\n",
    "\n",
    "                 brd_w = 10.,\n",
    "                 brd_sig = 5.,\n",
    "                 brd_len = 25,\n",
    "                 brd_exp = 1.,\n",
    "                 brd_offset = 1.,\n",
    "                 brd_fac = 0.,\n",
    "\n",
    "                 return_components = True,\n",
    "                )\n",
    "\n",
    "train_pars = dict(batch_size = 4, # Dataset batch size\n",
    "                  num_workers = 8, # Number of parallel processes to generate data\n",
    "                  checkpoint = 10, # Perform evaluation after that many batches\n",
    "                  n_eval = 10, # Number of batches in the evaluation\n",
    "                  max_checkpoints = 100, # Maximum number of checkpoints before finishing training\n",
    "                  out_dir = \"../data/Ensemble_PIPNet_test/\", # Output directory\n",
    "                  change_factor = {}, # Checkpoints where\n",
    "                  avg_models = False,\n",
    "                  device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                  monitor_end = \"\\r\"\n",
    "                 )\n",
    "\n",
    "model_pars = dict(n_models = 3,\n",
    "                  input_dim = 2,\n",
    "                  hidden_dim = 64,\n",
    "                  kernel_size = [[5, 5], [7, 7], [9, 9]],\n",
    "                  num_layers = 2,\n",
    "                  final_kernel_size = [1, 1, 1],\n",
    "                  batch_input = 2,\n",
    "                  bias = True,\n",
    "                  final_bias = True,\n",
    "                  return_all_layers = True,\n",
    "                  final_act = \"linear\",\n",
    "                  noise = 1.e-4,\n",
    "                 )\n",
    "    \n",
    "if not os.path.exists(train_pars[\"out_dir\"]):\n",
    "    os.mkdir(train_pars[\"out_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83ad5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.PIPDataset(**data_pars)\n",
    "\n",
    "net = model.ConvLSTMEnsemble(**model_pars).to(train_pars[\"device\"])\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "loss = model.CustomLoss(**loss_pars)\n",
    "\n",
    "sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc90942",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "    Training batch   10: loss =  8.5041e-01, mean loss =  1.0710e+00, lr =  1.0000e-03...\n",
      "  Checkpoint reached, evaluating the model...\n",
      "    Validation batch   10: loss =  8.6895e-01, mean loss =  9.6770e-01...\n",
      "  End of evaluation.\n",
      "    Training batch   20: loss =  7.1217e-01, mean loss =  8.9587e-01, lr =  1.0000e-03...\n",
      "  Checkpoint reached, evaluating the model...\n",
      "    Validation batch   10: loss =  6.4022e-01, mean loss =  7.3225e-01...\n",
      "  End of evaluation.\n",
      "    Training batch   30: loss =  9.4882e-01, mean loss =  9.2762e-01, lr =  1.0000e-03...\n",
      "  Checkpoint reached, evaluating the model...\n",
      "    Validation batch   10: loss =  8.9340e-01, mean loss =  7.5166e-01...\n",
      "  End of evaluation.\n",
      "    Training batch   40: loss =  4.8759e-01, mean loss =  7.2981e-01, lr =  1.0000e-03...\n",
      "  Checkpoint reached, evaluating the model...\n",
      "    Validation batch   10: loss =  7.6499e-01, mean loss =  9.2561e-01...\r"
     ]
    }
   ],
   "source": [
    "train.train(dataset, net, opt, loss, sch, train_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefabfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

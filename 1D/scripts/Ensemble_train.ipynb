{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23a758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import data\n",
    "import model\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29f555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pars = dict(\n",
    "                 # General parameters\n",
    "                 td = 256, # Number of points\n",
    "                 Fs = 12800, # Sampling frequency\n",
    "                 debug = False, #Â Print data generation details\n",
    "\n",
    "                 # Peak parameters\n",
    "                 pmin = 1, # Minimum number of Gaussians in a peak\n",
    "                 pmax = 1, # Maximum number of Gaussians in a peak\n",
    "                 ds = 0.03, # Spread of chemical shift values for each peak\n",
    "                 lw = [[5e1, 2e2], [1e2, 2e3]], # Linewidth range for Gaussians\n",
    "                 iso_p = [0.9, 0.1],\n",
    "                 iso_p_peakwise = True,\n",
    "                 phase = 0., # Spread of phase\n",
    "\n",
    "                 # Isotropic parameters\n",
    "                 nmin = 1, # Minimum number of peaks\n",
    "                 nmax = 15, # Maximum number of peaks\n",
    "                 shift_range = [2000., 10000.], # Chemical shift range\n",
    "                 positive = True, # Force the spectrum to be positive\n",
    "\n",
    "                 # MAS-dependent parameters\n",
    "                 mas_g_range = [[1e10, 1e11], [1e10, 5e11]], # MAS-dependent Gaussian broadening range\n",
    "                 mas_l_range = [[1e7, 1e8], [1e7, 5e8]], # MAS-dependent Lorentzian broadening range\n",
    "                 mas_s_range = [[-1e7, 1e7], [-1e7, 1e7]], # MAS-dependent shift range\n",
    "                 mas_p = [0.9, 0.1],\n",
    "                 mas_phase = 0.1, # Random phase range for MAS spectra\n",
    "                 peakwise_phase = True, # Whether the phase should be peak-wise or spectrum-wise\n",
    "                 encode_imag = False, # Encode the imaginary part of the MAS spectra\n",
    "                 nw = 4, # Number of MAS rates\n",
    "                 mas_w_range = [30000, 100000], # MAS rate range\n",
    "                 random_mas = True,\n",
    "                 encode_w = True, # Encode the MAS rate of the spectra\n",
    "\n",
    "                 # Post-processing parameters\n",
    "                 noise = 0., # Noise level\n",
    "                 smooth_end_len = 10, # Smooth ends of spectra\n",
    "                 scale_iso = 0.8, # Scale isotropic spectra\n",
    "                 offset = 0., # Baseline offset\n",
    "                 norm_wr = True, # Normalize MAS rate values\n",
    "                 wr_inv = False # Encode inverse of MAS rate instead of MAS rate\n",
    "                )\n",
    "\n",
    "loss_pars = dict(srp_w = 1.,\n",
    "                 srp_exp = 1.,\n",
    "                 srp_offset = 1.,\n",
    "                 srp_fac = 100.,\n",
    "\n",
    "                 brd_w = 10.,\n",
    "                 brd_sig = 5.,\n",
    "                 brd_len = 25,\n",
    "                 brd_exp = 1.,\n",
    "                 brd_offset = 1.,\n",
    "                 brd_fac = 0.,\n",
    "\n",
    "                 return_components = True,\n",
    "                )\n",
    "\n",
    "train_pars = dict(batch_size = 4, # Dataset batch size\n",
    "                  num_workers = 8, # Number of parallel processes to generate data\n",
    "                  checkpoint = 10, # Perform evaluation after that many batches\n",
    "                  n_eval = 10, # Number of batches in the evaluation\n",
    "                  max_checkpoints = 100, # Maximum number of checkpoints before finishing training\n",
    "                  out_dir = \"../data/Ensemble_PIPNet_test/\", # Output directory\n",
    "                  change_factor = {}, # Checkpoints where\n",
    "                  avg_models = False,\n",
    "                  device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                  monitor_end = \"\\r\"\n",
    "                 )\n",
    "\n",
    "model_pars = dict(n_models = 3,\n",
    "                  input_dim = 2,\n",
    "                  hidden_dim = 64,\n",
    "                  kernel_size = [[5, 5], [7, 7], [9, 9]],\n",
    "                  num_layers = 2,\n",
    "                  final_kernel_size = [1, 1, 1],\n",
    "                  batch_input = 2,\n",
    "                  bias = True,\n",
    "                  final_bias = True,\n",
    "                  return_all_layers = True,\n",
    "                  final_act = \"linear\",\n",
    "                  noise = 1.e-4,\n",
    "                 )\n",
    "    \n",
    "if not os.path.exists(train_pars[\"out_dir\"]):\n",
    "    os.mkdir(train_pars[\"out_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58c70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.PIPDataset(**data_pars)\n",
    "\n",
    "net = model.ConvLSTMEnsemble(**model_pars).to(train_pars[\"device\"])\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "loss = model.CustomLoss(**loss_pars)\n",
    "\n",
    "sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd9f59",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6648a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "    Training batch   10: loss =  1.0190e+00, mean loss =  1.0172e+00, lr =  1.0000e-03...\n",
      "  Checkpoint reached, evaluating the model...\n",
      "    Validation batch   10: loss =  8.0742e-01, mean loss =  8.6010e-01...\n",
      "  End of evaluation.\n",
      "    Training batch   20: loss =  6.4125e-01, mean loss =  8.0582e-01, lr =  1.0000e-03...\n",
      "  Checkpoint reached, evaluating the model...\n",
      "    Validation batch   10: loss =  9.9382e-01, mean loss =  8.6642e-01...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x123e6e790>\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  End of evaluation.\n",
      "    Training batch   25: loss =  9.1754e-01, mean loss =  7.9109e-01, lr =  1.0000e-03...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x123e6e790>\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"//anaconda3/envs/torch/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c33ce951c577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Work/PIP/PIPNet/1D/src/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, net, opt, loss, sch, train_pars)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.train(dataset, net, opt, loss, sch, train_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce33473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
